% ********************************************************************
% *                  Format for IMVIP 2017  papers,                  *
% *         based on the IMVIP 2001, 2006, 2014-2016 templates       *
% ********************************************************************
\documentclass[a4paper,11pt]{article}



\setlength{\topmargin}{-0.5cm}
\setlength{\headsep}{.5cm}
%\setlength{\footskip}{1.0cm}
\setlength{\textheight}{24cm}
\setlength{\textwidth}{17cm}
\setlength{\evensidemargin}{-.5cm}
\setlength{\oddsidemargin}{-.5cm}



\usepackage{fourier}
\usepackage{color}
\usepackage{graphicx}
 \graphicspath{{./Figs/}}
 \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\usepackage{url}
\usepackage[affil-it]{authblk}
\usepackage{amsmath}
%\usepackage{mathspec}
\usepackage{wrapfig}
\usepackage{xspace}

\usepackage[T1]{fontenc}
\usepackage{times}
\usepackage{verbatim}
\usepackage{subcaption}

\pagestyle{empty}

%%%%
\begin{document}

\title{Shape and Moment Invariants Local Descriptor for Structured Images}

\author{Anonymous Submission}
\affil{Anonymous Affiliation}

%\author{Elena Ranguelova}
%\affil{Netherlands eScience Centre\\ Amsterdam, The Netherlands.}
\date{}
\maketitle
\thispagestyle{empty}



\begin{abstract}
Finding correspondences between two images to determine if they depict the same scene or object, is a fundamental, yet challenging task. To cope with different viewpoints and lighting conditions, usually salient regions are detected as local invariant features, encoded by descriptors such as SIFT or SURF. While using the image intensities around a single point, the centroid of the region, to compute a histogram-of-gradients type descriptor, often works well, we argue that for structured scenes it is enough to use only the binary shape of the regions. We propose a $20$-dimensional Shape and Moment Invariant (SMI) descriptor and show that it outperforms the $64$-dimensional SURF on classical and transformation-independent datasets in terms of precision, achieving similar or even higher accuracy, while having a better scalability.
\end{abstract}
\textbf{Keywords:} image matching, affine-invariant descriptor, shape invariants, moment invariants



%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

\begin{wrapfigure}{r}{0.5\textwidth}
 \vspace{-22pt} 
\begin{center}
\begin{subfigure}[b]{0.22\textwidth}
  \includegraphics[width=\textwidth, height=\textwidth]{cool_car_scale5}
\end{subfigure}
\begin{subfigure}[b]{0.22\textwidth}
\includegraphics[width=\textwidth]{cool_car_viewpoint5}
\end{subfigure}
\end{center}
\vspace{-22pt}
\begin{center}
\begin{subfigure}[b]{0.22\textwidth}
  \includegraphics[width=\textwidth]{1graffiti_blur4}
\end{subfigure}
\begin{subfigure}[b]{0.22\textwidth}
\includegraphics[width=\textwidth]{2freiburg_center_blur4}
\end{subfigure}
\end{center}
\vspace{-20pt}
\caption{\small ``Is it the same object or scene?'' Matching two images under different transformations using MSER regions.\\ {\em Top image pair} (scale and viewpoint): SURF descriptor yields false negative (similarity score $0.096$), while SMI - true positive ($0.89$).
{\em Bottom image pair} (blur): SURF gives false positive ($0.27$), while SMI - true negative ($-0.11$).}
\label{fig:intro1}
  \vspace{-16pt}
\end{wrapfigure}

Automatically determining whether $2$ images depict partially the same physical scene is a fundamental computer vision problem such as baseline stereo matching, image retrieval, etc.~\cite{EscaleraCVPR07,Matas2002BMVC}). The used approach is {\em detection} of local (due to partial overlap) features, followed by matching of their {\em descriptors}. % {\em Detection} of local (due to partial overlap) features, followed by {\em matching} of their {\em descriptors}, is the used approach.
 Salient regions, corresponding to the same image patches, detected with high repeatability independently in each image are such features. Many detectors and descriptors are designed to be invariant to photometric (due to different sensors and lighting) and affine geometric transformations (due to different viewpoints). In recent years, a new approach of using large datasets of image patch correspondences is established, \cite{Snavely2008, ZagoruykoK15}. However, when very few, even only $2$ {\em structured} images, with homogeneous regions with distinctive boundaries, are available, e.~g.~in some scientific applications \cite{Ranguelova2016AICCSA}, deep learning is not applicable.

The Maximally Stable Extremal Regions (MSER) detector has become the standard in computer vision, \cite{ Matas2002BMVC}. It is often used in combination with a histogram-of-gradients type descriptor such as Scale-invariant Feature Transform (SIFT) or Speeded-Up Robust Features (SURF), \cite{Bay2008}, computed from image intensities around the centroids of the MSER regions. We argue that using the shape and moment information of the  regions encoded by a {\em Shape and Moment Invariants (SMI)} descriptor is beneficial, compared to image intensities around the region's centroid. Figure \ref{fig:intro1} illustrates two cases of image pairs, one depicting the same scene and the other- not, where SMI outperforms SURF applied on pre-detected MSER regions.

\section{Related work}
The literature describes large number of local detectors and descriptors, for a recent introduction and overview the reader is referred to \cite{Hassaballah:2016}. Here we mention very briefly only the closely related work.

%\paragraph{Salient region detectors}
A comparative performance evaluation of many detectors have concluded that the MSER is the best performing region detector for structured scenes, \cite{Mikolajczyk:2005}. Since, MSER has been integrated in MATLAB, OpenCV, VLFeat, etc.~, making it the default baseline detector. However, despite its success, the detector has several drawbacks, which have been addressed by improved detectors, including the Data-driven Morphology Salient Regions detector (DMSR), \cite{Ranguelova2016AICCSA}. Here, we propose to use a Binary detector (BIN) using the first step of DMSR construction: data-driven binarization explained in \cite{Ranguelova2016AICCSA}, with either all or only regions with large area ($A_{region} \ge f_A.A_{Image}$).

%\paragraph{Region descriptors}
Another comparative performance evaluation of many region descriptors have concluded that the "region-based SIFT descriptor" is best performing again for structured images, \cite{Mikolajczyk:descr:2005}. Since, we are interested in using the binary shape of the detected regions, we have chosen for the moment invariant theory. Moment invariants are a group of efficient shape descriptors. Flusser et al. developed a coherent theory and general framework for the derivation of Affine Moment Invariants (AMIs) using graph representation \cite{SukF04, Flusser09a}.

%\paragraph{Detector-descriptor combination}
Research has been performed not only to determine the best region detector and descriptor, but also the best combination detector - descriptor.
For example, the conclusion of the experiments in \cite{DahlAP11} is that the best combination is DOG or MSER detector and SIFT (SURF was not included in the experiments) or DAISY descriptors. SURF has been introduced as an improvement over SIFT and since has become a standard descriptor in many computer vision software libraries, making it the default baseline descriptor, \cite{Bay2008}.
Hence, we have chosen MSER - SURF as the baseline detector - descriptor combination.

\section{Image matching with Shape and Moment Invariant descriptor}\label{sec:match}
We propose a set of several Shape and Moment Invariants (SMI) derived from the binary shapes of the detected regions as a region descriptor. SMI descriptor contains {\em shape invariants} and {\em moment invariants}.

\paragraph{Shape invariants}
A binary shape of a region $R_i$ can be described by a set of simple properties of the original shape or the equivalent (up to the second order moments) ellipse $E_i$. These are: the region's area $a_i$, the area of the region's convex hull $a^c_i$, the length of the major and minor axes of $E_i$, $\mu_i$ and $\nu_i$ and the distance between the foci of the ellipse $\phi_i$. From these basic properties, a set of shape affine invariants are defined in Table \ref{tab:ssi}.   

\begin{table}[!ht]
\begin{center}
\begin{tabular}{|l||l|l|}
\hline
Invariant & Definition & Description\\
\hline
\hline
Relative Area & $\tilde{a}_i = {a_i}/{A}$ & region's area normalized by the image area $A$\\
\hline
Ratio Axes Lengths & $r_i = {\nu_i}/{\mu_i}$& ratio between $E_i$ minor and major axes lengths\\
%\hline
%Orientation & $o_i = \theta_i$& angle between the major axis and the $x$-axis of $E_i$ \\
\hline
Eccentricity &$e_i = \phi_i/\mu_i$& $e_i \in [0,1]$ (0 is a circle, 1 is a line segment.)\\
\hline
Solidity & $s_i = {a_i}/{a_i^c} $ & proportion of the convex hull pixels, that are also in the region. \\
\hline
\end{tabular}
\end{center}
\vspace{-20pt}
\caption{\small Simple shape invariants.} \label{tab:ssi}
  \vspace{-20pt}
\end{table}
%The simple shape invariants part of $SMI_i$ is then $S_i = \{\tilde{a}_i, r_i, e_i, s_i\}$.

\paragraph{Affine Moment Invariants}
 
If $f(x,y)$ is a real-valued image with $N$ points, the AMI functional is defined by
\begin{equation}
I(f) = \int_{-\infty}^{\infty} \prod_{k,j=1}^{N}C_{kj}^{n_{kj}} . \prod_{l=1}^{N}f(x_l, y_l)dx_ldy_l,
\end{equation}
where $n_{kj}$ are non-negative integers, $C_{kj} =x_ky_j - x_jy_k$ is the cross-product (graph edge) of points (nodes) $(x_k, y_k)$ and $(x_j, y_j)$, \cite{SukF04}. For full details of the AMI's theory the reader is referred to \cite{Flusser09a}.
We use the set of $16$ irreducible AMIs of $N=4$th order, $\{m_{ij},j=1 \ldots 16 \}$ , as implemented by the authors in an open source MATLAB software.% \cite{UTIAcode}. 
%The AMI part of $SMI_i$ is $M_i = \{m_{i1}, \ldots, m_{i16}\}$. 

Hence, the final descriptor for the $i$-th region is a $20$ element feature vector $SMI_i = \{\tilde{a}_i, r_i, e_i, s_i, m_{i1}, \ldots, m_{i16}\}$. 

\paragraph{Matching}

Lets $SMI1$ and $SMI2$ be $n1 \times 20$ and $n2 \times 20$ matrices, where each row is the SMI descriptor for the $n_1$ and $n_2$ regions detected via MSER or BIN (all/largest) detector in the pair of images $<I1, I2>$.
We compare exhaustively  $SMI1$ and $SMI2$ with Sum of square differences metric. The matching threshold for selection of the strongest matches is $mt$, the max ratio threshold for rejecting ambiguous matches is $mr$, the confidence of a match is $mc$ and only unique matches are allowed. Then, we select the top quality matches above a cost threshold $ct$. From those, we estimate in $it$ iterations the affine transformation ${\tilde T}$ between the two sets of points- centroids of the matching regions sets as average of $nr$ runs with allowed max point distance $md$. The two images are then transformed $J2 = {\tilde T}(I1)$, $J1 = {\tilde T}^{-1}(I2)$ and a correlation ($cor[X,Y] = cov[X,Y]/ \sqrt{var[X] var[Y]}$) between the original and transformed images is used for confirmation of a true match. If the average correlation similarity between both images and their transformed versions $(cor[I1, J1]+cor[I2, J2])/2$ is above a similarity threshold $st$, we declare the image pair $<I1, I2>$ to be depicting (partially) the same scene.

Figure \ref{fig:matching1} illustrates the major steps of the image matching using BIN + SMI in case of viewpoint distortion. Note the better alignment in the right part of the images due to the larger number of correct matches there. The steps are the same when using MSER instead of the data-driven binarization or SURF instead of SMI descriptor.

\begin{figure}[h]
 \vspace{-10pt} 
\begin{center}
\begin{subfigure}[b]{0.22\textwidth}
  \includegraphics[width=\textwidth]{graffiti1orig}
\end{subfigure}
\begin{subfigure}[b]{0.22\textwidth}
\includegraphics[width=\textwidth]{graffiti1orig_bin}
\end{subfigure}
\begin{subfigure}[b]{0.22\textwidth}
  \includegraphics[width=\textwidth]{graffiti_matched_12}
\end{subfigure}
\begin{subfigure}[b]{0.22\textwidth}
\includegraphics[width=\textwidth]{graffiti_overlay_12}
\end{subfigure}
\end{center}
\vspace{-22pt}
\begin{center}
\begin{subfigure}[b]{0.22\textwidth}
  \includegraphics[width=\textwidth]{graffiti1view}
\end{subfigure}
\begin{subfigure}[b]{0.22\textwidth}
\includegraphics[width=\textwidth]{graffiti1view_bin}
\end{subfigure}
\begin{subfigure}[b]{0.22\textwidth}
  \includegraphics[width=\textwidth]{graffiti_matched_21}
\end{subfigure}
\begin{subfigure}[b]{0.22\textwidth}
\includegraphics[width=\textwidth]{graffiti_overlay_21}
\end{subfigure}
\end{center}
\vspace{-20pt}
\caption{\small Matching two images of the same scene under viewpoint transformation using BIN regions and SMI descriptor. {\em First column:} original images $I1, I2$; {\em second column:} binarization; {\em third column:} matched BIN regions using SMI descriptor (blend view with pseudocolours); {\em fourth column:} overlay of the original and transformed images $(I1, J1), (I2, J2)$ }
\label{fig:matching1}
  \vspace{-12pt}
\end{figure}

\section{Performance Evaluation}

We have tested the performance of the MSER and BIN (all regions and only the largest) detectors in combinations with the SURF and SMI descriptors on $2$ datasets: Oxford (VGG), \cite{Mikolajczyk:2005} and OxFrei, \cite{Ranguelova2016AICCSA}. Each of the $4$ structured image sequences of the Oxford set consists of $1$ base and $5$ increasingly distorted images. Each sequence can be used to test only $1$ transformation $T$: viewpoint, scaling + rotation, decreased lighting and blur. OxFrei dataset overcomes this limitation: $9$ structured scenes each with $21$ images (original + $5$ images for $4 T$) using Oxford's real homographies. We compared all possible image pairs and assigned a flag {\em True/False} if a pair is depicting the same scene as described in Section \ref{sec:match}. The values of the parameters used in the experiments have been determined empirically: $mt=mr=1$, $f_A=2e-3$ (for BIN largest), $it=1000$, $nr=10$, $mc=95$, $md=8px$, $ct=0.025$, $st=0.25$.
%\paragraph{Oxford dataset}
Figure \ref{fig:intro1} illustrates the result for 2 pairs and Figure \ref{fig:matching2}- for all pairs (a pixel represents image pair and a square block - a sequence) of the OxFrei dataset. Note the lower number of false positives and correlation similarity variance when using the SMI descriptor.

\begin{figure}[h]
 \vspace{-10pt} 
\begin{center}
\begin{subfigure}[b]{0.18\textwidth}
  \includegraphics[width=\textwidth]{oxfrei_gt}
\end{subfigure}
\begin{subfigure}[b]{0.18\textwidth}
\includegraphics[width=\textwidth]{oxfrei_sim_mser_surf}
\end{subfigure}
\begin{subfigure}[b]{0.18\textwidth}
  \includegraphics[width=\textwidth]{oxfrei_mser_surf}
\end{subfigure}
\begin{subfigure}[b]{0.18\textwidth}
\includegraphics[width=\textwidth]{oxfrei_sim_mser_smi}
\end{subfigure}
\begin{subfigure}[b]{0.18\textwidth}
  \includegraphics[width=\textwidth]{oxfrei_mser_smi}
\end{subfigure}
\end{center}
\vspace{-20pt}
\caption{\small  All pairs of the OxFrei dataset: ``Is the image pair from the same scene?''. True(white)/False(black).  Correlation similarity: the lighter, the higher. {\em First:} ground truth; {\em second and third:} MSER + SURF; {\em fourth and fifth:} MSER + SMI.}
\label{fig:matching2}
  \vspace{-12pt}
\end{figure}

%% comment line

\begin{comment}
The performance results on the VGG dataset are summarized in Table \ref{tab:vgg}.
\begin{table}[!ht]
\begin{center}
\begin{tabular}{|l||c|c|c|c|c|c|c|}

\hline
Detector + descriptor & TP & TN & FP & FN & Accuracy &Precision &Recall\\
\hline
\hline
MSER + SURF & $128$ & $428$ &$4$ & $16$ & $0.965$ & $0.969$ & $0.889$\\
\hline
MSER + \bf{SMI} & $122$ &$430$  &$2$  &$22$  &$0.958$  & $0.98$ & $0.847$\\
\hline
BIN + SURF & $122$ & $426$ & $6$ & $22$ & $0.951$ & $0.953$ &$0.847$\\
\hline
BIN (All) + \bf{SMI} &$84$  &$432$  &$0$  &$60$ &$0.89$  & $1$ &$0.58$ \\
\hline
BIN (Largest) + \bf{SMI} &$112$  &$424$  &$8$  &$32$ &$0.93$  & $0.93$ &$0.77$ \\
\hline
\end{tabular}
\end{center}
\vspace{-20pt}
\caption{\small Performance of salient region detectors and descriptors on the VGG dataset.} \label{tab:vgg}
  \vspace{-10pt}
\end{table}

%\paragraph{OxFrei dataset}

The performance results on the OxFrei dataset are summarized in Table \ref{tab:oxfrei}.
\begin{table}[!ht]
\begin{center}
\begin{tabular}{|l||c|c|c|c|c|c|c|}

\hline
Detector + descriptor & TP & TN & FP & FN & Accuracy &Precision &Recall\\
\hline
\hline
MSER + SURF & $3309$ & $28848$ & $2904$ & $660$ & $0.90$ & $0.53$ & $0.83$\\
\hline
MSER + \bf{SMI} & $2957$ & $31162$ & $590$ & $1012$ & $0.95$ &$0.83$ & $0.74$\\
\hline
BIN + SURF & $2513$ & $28198$ & $3554$ & $1456$ & $0.85$ &$0.41$ & $0.63$\\
\hline
BIN (All) + \bf{SMI}  & $1275$ & $31298$ & $454$ & $2694$ & $0.91$ &$0.73$ & $0.32$\\
\hline
BIN (Largest) + \bf{SMI}  & $2079$& $28474$ & $3278$ & $1890$ & $0.85$ & $0.38$ & $0.52$\\
\hline
\end{tabular}
\end{center}
\vspace{-20pt}
\caption{Performance of salient region detectors and descriptors on the OxFrei dataset.} \label{tab:oxfrei}
  \vspace{-10pt}
\end{table}

\end{comment}

Table \ref{tab:perf} summarizes the perofrmance of the combinations of detectors and descriptors for the $2$ datasets. When using the default MSER detector, it seems beneficial to combine with an SMI instead of the standard SURF descriptor as in both datasets almost all performance measures are improved. The BIN (all) detector does not outperform MSER in the matching task and using only the largest regions in BIN (largest) improves the recall on the expense of the precision. The SMI descriptor achieves better precision in comparison to SURF independent on the detector or dataset. The best combination detector - descriptor is MSER - SMI when all measure are required to be high, especially accuracy and precision.

\begin{table}[!ht]
\begin{center}
  \vspace{-6pt}
\begin{tabular}{|l|*{6}{c|}}  % repeats {c|} 18 times
\hline
\multicolumn{1}{|r}{Dataset} & \multicolumn{3}{|c|}{Oxford}  & \multicolumn{3}{|c|}{OxFrei} \\ \hline
{Detector - descriptor}  & Accuracy      & Precision   &  Recall       & Accuracy        & Precision & Recall \\ \hline
MSER - SURF              & {\boldmath $0.965$} & $0.969$     & {\boldmath $0.889$} &  $0.90$         & $0.53$    & {\boldmath $0.83$}\\ \hline
MSER - \bf{SMI}          & {\boldmath $0.958$}       & {\boldmath $0.98$}&  {\boldmath$0.847$}       & {\boldmath $0.95$}    &{\boldmath $0.83$} &  {\boldmath$0.74$} \\ \hline
BIN (All) - SURF         & $0.951$       & $0.953$     &$0.847$        & $0.85$          & $0.41$    & $0.63$\\ \hline
BIN (All) - \bf{SMI}     & $0.89$        & {\boldmath $1$}   &$0.58$         & $0.91$          & $0.73$    & $0.32$\\ \hline
BIN (Largest) - \bf{SMI} & $0.93$        & $0.93$      &$0.77$         & $0.85$          & $0.38$    & $0.52$\\ \hline
\end{tabular}
\end{center}
\vspace{-20pt}
\caption{Performance of salient region detectors and descriptors on the Oxford and OxFrei datasets.} \label{tab:perf}
  \vspace{-10pt}
\end{table}

The developed MATLAB software will be released as open source with the final paper.
\section{Conclusion}
It is not possibly to use (deep) learning of image patches approach when trying to automatically determine if two images depict (partially) the same scene if only few images are available. It is beneficial not to discard the shape of the salient regions detected by the detector. For structured scenes, a descriptor based on the properties of the binary regions alone performs better than one based on image intensities.  The proposed shape and moment invariant, SMI, descriptor is a good choice when false positives should be minimal. In combination with the MSER detector, SMI achieves the highest precision and good accuracy and recall. In the future, the matching performance of the SMI descriptor should be tested on larger datasets.

%\section*{Acknowledgments}

%%\subsection{Bibliographic references}
%%References in a bib file format (e.g. imvip2017.bib given in the template) can be inserted using bibtex along with
%%\LaTeX\xspace/pdflatex (e.g.  \cite{hartley} or  \cite{jain, goodfellow}). 


%%%%%%%%%%%%%%%%%%%%%%%%
%\appendix

%\section{VGG dataset matching results }



%\section{OxFrei dataset matching results }


\footnotesize
\bibliographystyle{apalike}

\bibliography{imvip2017}


\end{document}

